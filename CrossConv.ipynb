{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5be9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6ff5b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
    "    stride = 1 if downsample == False else 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.GELU()\n",
    "    )\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "# PreNorm(inp, self.attn, nn.LayerNorm)\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, inp, oup, expansion=0.25):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        stride = 1 if self.downsample == False else 2\n",
    "        hidden_dim = int(inp * expansion)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                # down-sample in the first conv\n",
    "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
    "                          groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                SE(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        \n",
    "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            return self.proj(self.pool(x)) + self.conv(x)\n",
    "        else:\n",
    "            return x + self.conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(\n",
    "            0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(\n",
    "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
    "        else:\n",
    "            x = x + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerAlt(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "        \n",
    "        self.pre_attn = Rearrange('b c ih iw -> b (ih iw) c')\n",
    "        self.attn = nn.Sequential(\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm)\n",
    "        )\n",
    "        self.post_attn = Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            tmp = self.attn(self.pre_attn(self.pool2(x)))\n",
    "            print(tmp.shape)\n",
    "            x = self.proj(self.pool1(x)) + self.post_attn(tmp)\n",
    "        else:\n",
    "            x = x + self.post_attn(self.attn(self.pre_attn(x)))\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "feaacd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 128, 112, 112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7c32f060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12544, 128])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(x.shape[0], -1, x.shape[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6022af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossConv(pl.LightningModule):\n",
    "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=100):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        block = {'C': MBConv, 'T': TransformerAlt}\n",
    "\n",
    "        self.s0 = self._make_layer(\n",
    "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
    "        \n",
    "        self.s1_c = self._make_layer(\n",
    "            MBConv, channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
    "        self.s1_a = self._make_layer(\n",
    "            TransformerAlt, channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
    "        \n",
    "        self.s2_c = self._make_layer(\n",
    "            MBConv, channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
    "        self.s2_a = self._make_layer(\n",
    "            TransformerAlt, channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
    "            \n",
    "        self.s3_c = self._make_layer(\n",
    "            MBConv, channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
    "        self.s3_a = self._make_layer(\n",
    "            TransformerAlt, channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
    "        \n",
    "        self.s4_c = self._make_layer(\n",
    "            MBConv, channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
    "        self.s4_a = self._make_layer(\n",
    "            TransformerAlt, channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s0(x)\n",
    "        print(\"IN:\", x.shape)\n",
    "        x_c = self.s1_c(x)\n",
    "        x_a = self.s1_a(x)\n",
    "        print(\"OUTC_1: \", x_c.shape)\n",
    "        print(\"OUTA_1: \", x_a.shape)\n",
    "        # send x_c to x_a\n",
    "        x_a += x_c\n",
    "        \n",
    "        x_c = self.s2_c(x_c)\n",
    "        x_a = self.s2_a(x_a)\n",
    "        print(\"OUTC_2: \", x_c.shape)\n",
    "        print(\"OUTA_2: \", x_a.shape)\n",
    "        x_a += x_c\n",
    "        \n",
    "        x_c = self.s3_c(x_c)\n",
    "        x_a = self.s3_a(x_a)\n",
    "        print(\"OUTC_3: \", x_c.shape)\n",
    "        print(\"OUTA_3: \", x_a.shape)\n",
    "        x_a += x_c\n",
    "        \n",
    "        x_c = self.s4_c(x_c)\n",
    "        x_a = self.s4_a(x_a)\n",
    "        print(\"OUTC_4: \", x_c.shape)\n",
    "        print(\"OUTA_4: \", x_a.shape)\n",
    "        x_a += x_c\n",
    "\n",
    "        x = self.pool(x_a).view(-1, x_a.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
    "        layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(block(inp, oup, image_size, downsample=True))\n",
    "            else:\n",
    "                layers.append(block(oup, oup, image_size))\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1a6c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossConv((224,224), 3, [2,2,3,5,2], [64,96, 192, 384, 768]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2096c9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: torch.Size([1, 64, 112, 112])\n",
      "torch.Size([1, 3136, 96])\n",
      "OUTC_1:  torch.Size([1, 96, 56, 56])\n",
      "OUTA_1:  torch.Size([1, 96, 56, 56])\n",
      "torch.Size([1, 784, 192])\n",
      "OUTC_2:  torch.Size([1, 192, 28, 28])\n",
      "OUTA_2:  torch.Size([1, 192, 28, 28])\n",
      "torch.Size([1, 196, 384])\n",
      "OUTC_3:  torch.Size([1, 384, 14, 14])\n",
      "OUTA_3:  torch.Size([1, 384, 14, 14])\n",
      "torch.Size([1, 49, 768])\n",
      "OUTC_4:  torch.Size([1, 768, 7, 7])\n",
      "OUTA_4:  torch.Size([1, 768, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.7863e-01,  3.5478e-01, -2.0593e+00,  2.1483e+00, -3.0856e+00,\n",
       "          4.0933e-01,  1.2546e+00, -1.0337e+00,  2.7871e+00, -3.5449e+00,\n",
       "          1.4353e+00, -1.8868e+00,  1.0398e+00, -1.5077e+00,  3.2755e+00,\n",
       "          1.5785e-01, -3.7156e+00,  1.2384e+00,  3.5044e+00,  4.2140e+00,\n",
       "          2.1337e+00,  3.9813e+00, -2.8954e-01, -6.1068e-01,  1.1019e+00,\n",
       "          2.6909e+00, -3.1690e+00, -3.0777e+00,  1.4917e+00, -1.1624e+00,\n",
       "         -6.7580e-01,  1.1943e+00, -2.8423e-01, -4.7585e-01,  1.2595e+00,\n",
       "         -4.1808e+00, -1.0582e+00,  1.5740e-01,  8.1200e-01,  9.0934e-02,\n",
       "         -1.7720e+00,  8.1646e-01,  1.5679e+00, -1.7271e+00,  4.0082e+00,\n",
       "          3.0562e+00, -1.5739e+00, -2.0244e+00, -5.4750e-01, -2.6108e+00,\n",
       "          3.1223e-01,  4.3318e+00,  2.7367e+00,  1.8563e-03, -3.3009e+00,\n",
       "         -1.8327e-01, -2.7205e-01, -1.3748e+00,  2.4828e+00, -4.1802e+00,\n",
       "          5.1154e+00, -2.2961e+00, -4.3411e-01,  8.1770e-01,  2.4382e-01,\n",
       "         -2.9314e+00, -2.2150e+00,  1.1093e+00,  4.3148e+00, -3.3620e+00,\n",
       "          2.7634e+00, -1.2851e+00,  9.4251e-01, -4.5856e-01,  1.3854e+00,\n",
       "         -1.9453e+00,  4.1276e+00,  3.0883e+00, -5.5050e-01,  4.4707e+00,\n",
       "         -2.0060e+00,  2.9214e+00,  4.1973e+00, -1.8678e+00,  9.5026e-01,\n",
       "          2.6912e+00,  4.4931e-01, -3.3222e+00,  4.4248e+00, -9.1157e-01,\n",
       "         -2.3702e+00,  6.7229e-01,  5.9921e-01,  2.2739e+00, -4.9408e+00,\n",
       "          2.3626e+00,  6.7065e-01, -4.8000e-01, -2.0249e-01, -6.0623e+00]],\n",
       "       device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand(1,3,224,224).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a451461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371dd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
